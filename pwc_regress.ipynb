{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Josh Barrios\n",
    "Deep CNN for ECG/PCW regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random, math\n",
    "import sys, os, time, json\n",
    "import scipy.interpolate as interp \n",
    "import csv \n",
    "import copy\n",
    "import timeit\n",
    "import pandas as pd\n",
    "import re\n",
    "import pdb\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-25T22:34:05+00:00\n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 7.1.1\n",
      "\n",
      "compiler   : GCC 5.4.0 20160609\n",
      "system     : Linux\n",
      "release    : 3.10.0-693.21.1.el7.x86_64\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 48\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json       2.0.9\n",
      "tensorflow 1.12.0\n",
      "numpy      1.15.4\n",
      "pandas     0.23.4\n",
      "re         2.2.1\n",
      "csv        1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark  --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2344/2344 [02:05<00:00, 18.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to find: []\n",
      "Num ECGS extracted: 2344\n"
     ]
    }
   ],
   "source": [
    "ecg_directory = '/volume/ECG/pcw_regression/run1_ph_csvs'\n",
    "file_dict = {}\n",
    "fail = []\n",
    "\n",
    "for filename in tqdm(os.listdir(ecg_directory)):\n",
    "    name = filename[:-4]\n",
    "    if name not in file_dict.keys():\n",
    "        try:\n",
    "            file_dict[name] = [-1,1]\n",
    "            ecg_raw = np.loadtxt(ecg_directory + '/' + filename, skiprows=1, delimiter=',', usecols=range(12))\n",
    "            len_leads = len(ecg_raw)\n",
    "            if len_leads == 5000:\n",
    "                file_dict[name][0] = ecg_raw[::2]\n",
    "            elif len_leads == 2500:\n",
    "                file_dict[name][0] = ecg_raw\n",
    "            else:\n",
    "                print('len_leads:', len_leads)\n",
    "        except:\n",
    "            fail.append(filename)\n",
    "\n",
    "print('Failed to find:', fail)\n",
    "print('Num ECGS extracted:',len(file_dict))\n",
    "np.save('an7_v1.npy',file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(lab_value):\n",
    "#     ecg_directory = '/gunvant_nas_trop/ecgs/%s' % lab_value\n",
    "#     file_dict = {}\n",
    "#     fail = []\n",
    "    \n",
    "#     for filename in tqdm(os.listdir(ecg_directory)):\n",
    "#         name = filename[:-4]\n",
    "#         if name not in file_dict.keys():\n",
    "#             try:\n",
    "# #                 pdb.set_trace()\n",
    "#                 file_dict[name] = [-1,1]\n",
    "#                 ecg_raw = np.loadtxt(ecg_directory + '/' + filename, skiprows=1, delimiter=',', usecols=range(12))\n",
    "#                 len_leads = len(ecg_raw)\n",
    "#                 if len_leads == 5000:\n",
    "#                     file_dict[name][0] = ecg_raw[::2]\n",
    "#                 elif len_leads == 2500:\n",
    "#                     file_dict[name][0] = ecg_raw\n",
    "#                 else:\n",
    "#                     print('len_leads:', len_leads)\n",
    "#             except:\n",
    "#                 fail.append(filename)\n",
    "# #         if len(file_dict)>100:\n",
    "# #             break;\n",
    "\n",
    "#     print('Failed to find:', fail)\n",
    "#     print('Num ECGS extracted:',len(file_dict))\n",
    "#     np.save('ef.npy',file_dict)\n",
    "\n",
    "    file_dict = np.load('an7_v1.npy',allow_pickle=True,encoding='latin1')\n",
    "    file_dict = file_dict.item()\n",
    "\n",
    "    train_voltages = []\n",
    "    train_labels = []\n",
    "    val_voltages = []\n",
    "    val_labels = []\n",
    "    test_voltages = []\n",
    "    test_labels = []\n",
    "    \n",
    "    train_labels_cat = []\n",
    "    val_labels_cat = []\n",
    "    test_labels_cat = []\n",
    "    \n",
    "    lab_values = pd.read_csv('/volume/ECG/pcw_regression/run1_uu_mrn.csv')\n",
    "\n",
    "    mrns = lab_values['mrn'].drop_duplicates()\n",
    "    lab_values = lab_values.set_index('UUID')\n",
    "    mrn_train,mrn_not_train = train_test_split(mrns,test_size=0.3, random_state = 111)\n",
    "    mrn_val, mrn_test = train_test_split(mrn_not_train,test_size=0.6667, random_state = 111)\n",
    "    \n",
    "    CUTOFFS = [0, 30, 100]\n",
    "    \n",
    "    train_dems = [] # [gender, age, weight, height, bsa]\n",
    "    val_dems = []\n",
    "    test_dems = []\n",
    "    \n",
    "    \n",
    "    failed = []\n",
    "    excluded = []\n",
    "    for mrn in tqdm(mrn_train.values):\n",
    "        for key in lab_values[lab_values['mrn'] == mrn].index:\n",
    "            try:\n",
    "                label = float(lab_values.loc[key,'disease'])\n",
    "    #             this_hr = get_hr(file_dict[key][0])\n",
    "    #             if not math.isnan(this_hr):\n",
    "                if label>0.0:\n",
    "\n",
    "    #                 this_vec = []\n",
    "    #                 this_gen = lab_values.loc[[key],'Gender_y'][0]\n",
    "    #                 if this_gen == 'Female':\n",
    "    #                     this_vec.append(0)\n",
    "    #                 else:\n",
    "    #                     this_vec.append(1)\n",
    "\n",
    "    #                 this_vec.append(lab_values.loc[[key],'age'][0])\n",
    "    #                 this_vec.append(lab_values.loc[[key],'WeightKG'][0])\n",
    "    #                 this_vec.append(lab_values.loc[[key],'HeightCM'][0])\n",
    "    #                 this_vec.append(lab_values.loc[[key],'BSA'][0])\n",
    "\n",
    "    #                 train_dems.append(this_vec)\n",
    "\n",
    "    #                 if label<=0.1:\n",
    "    #                     label=0.1\n",
    "                    train_voltages.append(file_dict[key][0])\n",
    "\n",
    "\n",
    "                    train_labels.append([label])\n",
    "                    for ii in range(1, len(CUTOFFS)):\n",
    "                        if label<CUTOFFS[ii] and label>=CUTOFFS[ii-1]:\n",
    "                            train_labels_cat.append(ii-1)\n",
    "\n",
    "    #                 train_labels.append(class_vec)\n",
    "\n",
    "    #                 train_labels_cat.append([label])\n",
    "    #                 train_labels.append(this_hr)\n",
    "                else:\n",
    "                    excluded.append(key)\n",
    "    #                 pdb.set_trace() \n",
    "                    pass\n",
    "            except:\n",
    "                failed.append(key)\n",
    "    #             traceback.print_exc()\n",
    "    #             pdb.set_trace() \n",
    "                pass\n",
    "    \n",
    "    for mrn in tqdm(mrn_val.values):\n",
    "        for key in lab_values[lab_values['mrn'] == mrn].index:\n",
    "            try:\n",
    "                label = float(lab_values.loc[key,'disease'])\n",
    "    #             this_hr = get_hr(file_dict[key][0])\n",
    "    #             if not math.isnan(this_hr): \n",
    "                if label>=0.0:# or random.random()<0.2:\n",
    "    #                 this_vec = []\n",
    "    #                 this_gen = lab_values.loc[[key],'Gender_y'][0]\n",
    "    #                 if this_gen == 'Female':\n",
    "    #                     this_vec.append(0)\n",
    "    #                 else:\n",
    "    #                     this_vec.append(1)\n",
    "\n",
    "    #                 this_vec.append(lab_values.loc[[key],'age'][0])\n",
    "    #                 this_vec.append(lab_values.loc[[key],'WeightKG'][0])\n",
    "    #                 this_vec.append(lab_values.loc[[key],'HeightCM'][0])\n",
    "    #                 this_vec.append(lab_values.loc[[key],'BSA'][0])\n",
    "\n",
    "    #                 val_dems.append(this_vec)\n",
    "\n",
    "    #                 if label<=0.1:\n",
    "    #                     label=0.1\n",
    "                    val_voltages.append(file_dict[key][0])\n",
    "\n",
    "\n",
    "                    val_labels.append([label])\n",
    "\n",
    "                    for ii in range(1, len(CUTOFFS)):\n",
    "                        if label<CUTOFFS[ii] and label>=CUTOFFS[ii-1]:\n",
    "                            val_labels_cat.append(ii-1)\n",
    "\n",
    "    #                 val_labels.append(class_vec)\n",
    "\n",
    "    #                 if label <= CUTOFF:\n",
    "    #                     val_labels.append([0])\n",
    "    #                 else:\n",
    "    #                     val_labels.append([1])\n",
    "    #                 val_labels_raw.append([label])\n",
    "    #                 val_labels.append(this_hr)\n",
    "                else:\n",
    "                    excluded.append(key)\n",
    "    #                     pdb.set_trace() \n",
    "                    pass\n",
    "            except:\n",
    "                failed.append(key)\n",
    "                pass\n",
    "\n",
    "    for mrn in tqdm(mrn_test.values):\n",
    "        for key in lab_values[lab_values['mrn'] == mrn].index:\n",
    "            try:\n",
    "                label = float(lab_values.loc[key,'disease'])\n",
    "    #             this_hr = get_hr(file_dict[key][0])\n",
    "    #             if not math.isnan(this_hr):\n",
    "                if label>=0.0:# or random.random()<0.2:\n",
    "\n",
    "    #                 this_vec = []\n",
    "    #                 this_gen = lab_values.loc[[key],'Gender_y'][0]\n",
    "    #                 if this_gen == 'Female':\n",
    "    #                     this_vec.append(0)\n",
    "    #                 else:\n",
    "    #                     this_vec.append(1)\n",
    "\n",
    "    #                 this_vec.append(lab_values.loc[[key],'age'][0])\n",
    "    #                 this_vec.append(lab_values.loc[[key],'WeightKG'][0])\n",
    "    #                 this_vec.append(lab_values.loc[[key],'HeightCM'][0])\n",
    "    #                 this_vec.append(lab_values.loc[[key],'BSA'][0])\n",
    "\n",
    "    #                 test_dems.append(this_vec)\n",
    "\n",
    "\n",
    "    #                 if label<=0.1:\n",
    "    #                     label=0.1\n",
    "                    test_voltages.append(file_dict[key][0])\n",
    "\n",
    "                    test_labels.append([label])\n",
    "\n",
    "                    for ii in range(1, len(CUTOFFS)):\n",
    "                        if label<CUTOFFS[ii] and label>=CUTOFFS[ii-1]:\n",
    "                            test_labels_cat.append(ii-1)\n",
    "\n",
    "    #                 test_labels.append(class_vec)\n",
    "\n",
    "    #                 if label <= CUTOFF:\n",
    "    #                     test_labels.append([0])\n",
    "    #                 else:\n",
    "    #                     test_labels.append([1])\n",
    "\n",
    "    #                 test_labels_cat.append([label])\n",
    "    #                 test_labels.append(this_hr)\n",
    "                else:\n",
    "                    excluded.append(key)\n",
    "    #                     pdb.set_trace() \n",
    "                    pass\n",
    "            except:\n",
    "                failed.append(key)\n",
    "                pass\n",
    "    \n",
    "    print('Excluded:', len(excluded))\n",
    "\n",
    "#     labels_max = 1 # log10(1 + 10)\n",
    "#     labels_min = -2\n",
    "\n",
    "#     x_train = np.swapaxes(train_voltages,1,2)\n",
    "#     x_train = ( ( np.clip(train_voltages, global_min, global_max) - global_min ) / (global_max - global_min) ).reshape((-1,2500,12))\n",
    "    x_train = np.array(train_voltages)\n",
    "    pt = PowerTransformer()\n",
    "    train_labels = np.clip(train_labels,0.01,50)\n",
    "    pt.fit(train_labels)\n",
    "    y_train = pt.transform(train_labels)\n",
    "#     y_train = y_train*2-1\n",
    "#     y_train = np.array(train_labels)\n",
    "#     y_train = (np.array(train_labels).reshape((-1,1)) - labels_min) / (labels_max - labels_min)\n",
    "#     y_train = (np.array(train_labels).reshape((-1,1)) - labels_mean) / (labels_std)\n",
    "\n",
    "    \n",
    "#     x_val = np.swapaxes(val_voltages,1,2)\n",
    "#     x_val = ( ( np.clip(val_voltages, global_min, global_max) - global_min ) / (global_max - global_min) ).reshape((-1,2500,12))\n",
    "    x_val = np.array(val_voltages)\n",
    "    val_labels = np.clip(val_labels,0.01,50)\n",
    "    print(val_labels)\n",
    "    y_val = pt.transform(val_labels)\n",
    "    print(y_val)\n",
    "#     y_val = (np.log10(np.clip(val_labels,0.01,86)) - labels_min) / (labels_max - labels_min)\n",
    "#     y_val = y_val*2-1\n",
    "#     y_val = np.array(val_labels)\n",
    "#     y_val = (np.array(val_labels).reshape((-1,1))  - labels_min) / (labels_max - labels_min)\n",
    "#     y_val = (np.array(val_labels).reshape((-1,1))  - labels_mean) / (labels_std)\n",
    "\n",
    "#     x_test = np.swapaxes(test_voltages,1,2)\n",
    "#     x_test = ( ( np.clip(test_voltages, global_min, global_max) - global_min ) / (global_max - global_min) ).reshape((-1,2500,12))\n",
    "    x_test = np.array(test_voltages)\n",
    "    test_labels = np.clip(test_labels,0.01,50)\n",
    "    y_test = pt.transform(test_labels)\n",
    "#     y_test = (np.log10(np.clip(test_labels,0.01,86)) - labels_min) / (labels_max - labels_min)\n",
    "#     y_test = y_test*2-1\n",
    "#     y_test = np.array(test_labels)\n",
    "#     y_test = (np.array(test_labels).reshape((-1,1))  - labels_min) / (labels_max - labels_min)\n",
    "#     y_test = (np.array(test_labels).reshape((-1,1))  - labels_mean) / (labels_std)\n",
    "\n",
    "    \n",
    "#     pdb.set_trace() \n",
    "\n",
    "    print(x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape, np.array(train_dems).shape, np.array(val_dems).shape, np.array(test_dems).shape)\n",
    "\n",
    "    return pt, train_labels, x_train, x_val, x_test, y_train, y_val, y_test, np.array(train_dems), np.array(val_dems), np.array(test_dems), train_labels_cat, val_labels_cat, test_labels_cat\n",
    "    \n",
    "\n",
    "def get_hr(ecg_raw):\n",
    "    hrs = []\n",
    "    _,_,_,_,_,_,hrs = ecg.ecg(signal=ecg_raw[:,1], sampling_rate=250, show=False)\n",
    "    \n",
    "    return np.mean(hrs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1640/1640 [00:01<00:00, 1472.83it/s]\n",
      "100%|██████████| 234/234 [00:00<00:00, 1521.73it/s]\n",
      "100%|██████████| 470/470 [00:00<00:00, 1490.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluded: 7\n",
      "[[30.]\n",
      " [21.]\n",
      " [11.]\n",
      " [12.]\n",
      " [12.]\n",
      " [16.]\n",
      " [ 5.]\n",
      " [18.]\n",
      " [15.]\n",
      " [13.]\n",
      " [ 9.]\n",
      " [26.]\n",
      " [16.]\n",
      " [ 6.]\n",
      " [26.]\n",
      " [30.]\n",
      " [ 8.]\n",
      " [33.]\n",
      " [24.]\n",
      " [17.]\n",
      " [ 7.]\n",
      " [12.]\n",
      " [29.]\n",
      " [ 8.]\n",
      " [20.]\n",
      " [13.]\n",
      " [31.]\n",
      " [14.]\n",
      " [19.]\n",
      " [16.]\n",
      " [14.]\n",
      " [20.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [ 7.]\n",
      " [ 1.]\n",
      " [ 5.]\n",
      " [14.]\n",
      " [ 2.]\n",
      " [11.]\n",
      " [24.]\n",
      " [18.]\n",
      " [24.]\n",
      " [23.]\n",
      " [13.]\n",
      " [19.]\n",
      " [29.]\n",
      " [ 5.]\n",
      " [10.]\n",
      " [17.]\n",
      " [13.]\n",
      " [11.]\n",
      " [22.]\n",
      " [10.]\n",
      " [20.]\n",
      " [ 5.]\n",
      " [22.]\n",
      " [11.]\n",
      " [18.]\n",
      " [ 9.]\n",
      " [ 5.]\n",
      " [32.]\n",
      " [14.]\n",
      " [12.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [20.]\n",
      " [21.]\n",
      " [ 4.]\n",
      " [19.]\n",
      " [16.]\n",
      " [19.]\n",
      " [17.]\n",
      " [10.]\n",
      " [19.]\n",
      " [11.]\n",
      " [13.]\n",
      " [ 4.]\n",
      " [26.]\n",
      " [ 5.]\n",
      " [19.]\n",
      " [18.]\n",
      " [ 8.]\n",
      " [15.]\n",
      " [12.]\n",
      " [ 7.]\n",
      " [ 6.]\n",
      " [11.]\n",
      " [29.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [ 5.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [13.]\n",
      " [24.]\n",
      " [17.]\n",
      " [10.]\n",
      " [20.]\n",
      " [ 8.]\n",
      " [11.]\n",
      " [19.]\n",
      " [10.]\n",
      " [17.]\n",
      " [ 8.]\n",
      " [ 5.]\n",
      " [27.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [ 9.]\n",
      " [10.]\n",
      " [15.]\n",
      " [ 7.]\n",
      " [11.]\n",
      " [15.]\n",
      " [29.]\n",
      " [16.]\n",
      " [ 6.]\n",
      " [33.]\n",
      " [ 7.]\n",
      " [24.]\n",
      " [16.]\n",
      " [13.]\n",
      " [20.]\n",
      " [14.]\n",
      " [ 3.]\n",
      " [ 8.]\n",
      " [15.]\n",
      " [11.]\n",
      " [11.]\n",
      " [ 7.]\n",
      " [17.]\n",
      " [10.]\n",
      " [10.]\n",
      " [30.]\n",
      " [ 9.]\n",
      " [ 7.]\n",
      " [ 4.]\n",
      " [20.]\n",
      " [ 7.]\n",
      " [21.]\n",
      " [12.]\n",
      " [ 7.]\n",
      " [20.]\n",
      " [15.]\n",
      " [11.]\n",
      " [12.]\n",
      " [19.]\n",
      " [12.]\n",
      " [13.]\n",
      " [11.]\n",
      " [13.]\n",
      " [29.]\n",
      " [28.]\n",
      " [28.]\n",
      " [23.]\n",
      " [19.]\n",
      " [ 2.]\n",
      " [ 8.]\n",
      " [ 5.]\n",
      " [19.]\n",
      " [13.]\n",
      " [10.]\n",
      " [18.]\n",
      " [14.]\n",
      " [10.]\n",
      " [ 4.]\n",
      " [19.]\n",
      " [11.]\n",
      " [36.]\n",
      " [22.]\n",
      " [10.]\n",
      " [ 4.]\n",
      " [16.]\n",
      " [30.]\n",
      " [27.]\n",
      " [19.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [18.]\n",
      " [17.]\n",
      " [12.]\n",
      " [22.]\n",
      " [12.]\n",
      " [11.]\n",
      " [17.]\n",
      " [30.]\n",
      " [ 5.]\n",
      " [14.]\n",
      " [14.]\n",
      " [20.]\n",
      " [20.]\n",
      " [34.]\n",
      " [ 5.]\n",
      " [13.]\n",
      " [14.]\n",
      " [ 4.]\n",
      " [13.]\n",
      " [14.]\n",
      " [20.]\n",
      " [ 8.]\n",
      " [ 6.]\n",
      " [ 9.]\n",
      " [14.]\n",
      " [15.]\n",
      " [18.]\n",
      " [ 3.]\n",
      " [ 3.]\n",
      " [23.]\n",
      " [19.]\n",
      " [24.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [22.]\n",
      " [ 6.]\n",
      " [28.]\n",
      " [10.]\n",
      " [ 8.]\n",
      " [17.]\n",
      " [24.]\n",
      " [18.]\n",
      " [ 8.]\n",
      " [28.]\n",
      " [13.]\n",
      " [19.]\n",
      " [ 5.]\n",
      " [12.]\n",
      " [13.]\n",
      " [24.]\n",
      " [30.]\n",
      " [ 4.]\n",
      " [11.]\n",
      " [11.]\n",
      " [22.]]\n",
      "[[ 1.65991961]\n",
      " [ 0.86966259]\n",
      " [-0.32931499]\n",
      " [-0.1841502 ]\n",
      " [-0.1841502 ]\n",
      " [ 0.33076222]\n",
      " [-1.44069733]\n",
      " [ 0.55772969]\n",
      " [ 0.21045274]\n",
      " [-0.04635033]\n",
      " [-0.64624879]\n",
      " [ 1.33097835]\n",
      " [ 0.33076222]\n",
      " [-1.21465366]\n",
      " [ 1.33097835]\n",
      " [ 1.65991961]\n",
      " [-0.82104219]\n",
      " [ 1.88822311]\n",
      " [ 1.15401755]\n",
      " [ 0.44637808]\n",
      " [-1.00950893]\n",
      " [-0.1841502 ]\n",
      " [ 1.58050792]\n",
      " [-0.82104219]\n",
      " [ 0.76907094]\n",
      " [-0.04635033]\n",
      " [ 1.73761801]\n",
      " [ 0.08494682]\n",
      " [ 0.66518692]\n",
      " [ 0.33076222]\n",
      " [ 0.08494682]\n",
      " [ 0.76907094]\n",
      " [-0.48290531]\n",
      " [-0.82104219]\n",
      " [-1.00950893]\n",
      " [-2.762578  ]\n",
      " [-1.44069733]\n",
      " [ 0.08494682]\n",
      " [-2.32851471]\n",
      " [-0.32931499]\n",
      " [ 1.15401755]\n",
      " [ 0.55772969]\n",
      " [ 1.15401755]\n",
      " [ 1.06192913]\n",
      " [-0.04635033]\n",
      " [ 0.66518692]\n",
      " [ 1.58050792]\n",
      " [-1.44069733]\n",
      " [-0.48290531]\n",
      " [ 0.44637808]\n",
      " [-0.04635033]\n",
      " [-0.32931499]\n",
      " [ 0.96720905]\n",
      " [-0.48290531]\n",
      " [ 0.76907094]\n",
      " [-1.44069733]\n",
      " [ 0.96720905]\n",
      " [-0.32931499]\n",
      " [ 0.55772969]\n",
      " [-0.64624879]\n",
      " [-1.44069733]\n",
      " [ 1.81369195]\n",
      " [ 0.08494682]\n",
      " [-0.1841502 ]\n",
      " [-0.82104219]\n",
      " [-1.21465366]\n",
      " [ 0.76907094]\n",
      " [ 0.86966259]\n",
      " [-1.69387693]\n",
      " [ 0.66518692]\n",
      " [ 0.33076222]\n",
      " [ 0.66518692]\n",
      " [ 0.44637808]\n",
      " [-0.48290531]\n",
      " [ 0.66518692]\n",
      " [-0.32931499]\n",
      " [-0.04635033]\n",
      " [-1.69387693]\n",
      " [ 1.33097835]\n",
      " [-1.44069733]\n",
      " [ 0.66518692]\n",
      " [ 0.55772969]\n",
      " [-0.82104219]\n",
      " [ 0.21045274]\n",
      " [-0.1841502 ]\n",
      " [-1.00950893]\n",
      " [-1.21465366]\n",
      " [-0.32931499]\n",
      " [ 1.58050792]\n",
      " [-0.82104219]\n",
      " [-1.21465366]\n",
      " [-1.44069733]\n",
      " [-1.44069733]\n",
      " [-1.21465366]\n",
      " [-0.04635033]\n",
      " [ 1.15401755]\n",
      " [ 0.44637808]\n",
      " [-0.48290531]\n",
      " [ 0.76907094]\n",
      " [-0.82104219]\n",
      " [-0.32931499]\n",
      " [ 0.66518692]\n",
      " [-0.48290531]\n",
      " [ 0.44637808]\n",
      " [-0.82104219]\n",
      " [-1.44069733]\n",
      " [ 1.41614829]\n",
      " [-0.82104219]\n",
      " [-0.64624879]\n",
      " [-0.64624879]\n",
      " [-0.48290531]\n",
      " [ 0.21045274]\n",
      " [-1.00950893]\n",
      " [-0.32931499]\n",
      " [ 0.21045274]\n",
      " [ 1.58050792]\n",
      " [ 0.33076222]\n",
      " [-1.21465366]\n",
      " [ 1.88822311]\n",
      " [-1.00950893]\n",
      " [ 1.15401755]\n",
      " [ 0.33076222]\n",
      " [-0.04635033]\n",
      " [ 0.76907094]\n",
      " [ 0.08494682]\n",
      " [-1.98406389]\n",
      " [-0.82104219]\n",
      " [ 0.21045274]\n",
      " [-0.32931499]\n",
      " [-0.32931499]\n",
      " [-1.00950893]\n",
      " [ 0.44637808]\n",
      " [-0.48290531]\n",
      " [-0.48290531]\n",
      " [ 1.65991961]\n",
      " [-0.64624879]\n",
      " [-1.00950893]\n",
      " [-1.69387693]\n",
      " [ 0.76907094]\n",
      " [-1.00950893]\n",
      " [ 0.86966259]\n",
      " [-0.1841502 ]\n",
      " [-1.00950893]\n",
      " [ 0.76907094]\n",
      " [ 0.21045274]\n",
      " [-0.32931499]\n",
      " [-0.1841502 ]\n",
      " [ 0.66518692]\n",
      " [-0.1841502 ]\n",
      " [-0.04635033]\n",
      " [-0.32931499]\n",
      " [-0.04635033]\n",
      " [ 1.58050792]\n",
      " [ 1.49928611]\n",
      " [ 1.49928611]\n",
      " [ 1.06192913]\n",
      " [ 0.66518692]\n",
      " [-2.32851471]\n",
      " [-0.82104219]\n",
      " [-1.44069733]\n",
      " [ 0.66518692]\n",
      " [-0.04635033]\n",
      " [-0.48290531]\n",
      " [ 0.55772969]\n",
      " [ 0.08494682]\n",
      " [-0.48290531]\n",
      " [-1.69387693]\n",
      " [ 0.66518692]\n",
      " [-0.32931499]\n",
      " [ 2.10328518]\n",
      " [ 0.96720905]\n",
      " [-0.48290531]\n",
      " [-1.69387693]\n",
      " [ 0.33076222]\n",
      " [ 1.65991961]\n",
      " [ 1.41614829]\n",
      " [ 0.66518692]\n",
      " [-0.82104219]\n",
      " [-0.64624879]\n",
      " [ 0.55772969]\n",
      " [ 0.44637808]\n",
      " [-0.1841502 ]\n",
      " [ 0.96720905]\n",
      " [-0.1841502 ]\n",
      " [-0.32931499]\n",
      " [ 0.44637808]\n",
      " [ 1.65991961]\n",
      " [-1.44069733]\n",
      " [ 0.08494682]\n",
      " [ 0.08494682]\n",
      " [ 0.76907094]\n",
      " [ 0.76907094]\n",
      " [ 1.96128682]\n",
      " [-1.44069733]\n",
      " [-0.04635033]\n",
      " [ 0.08494682]\n",
      " [-1.69387693]\n",
      " [-0.04635033]\n",
      " [ 0.08494682]\n",
      " [ 0.76907094]\n",
      " [-0.82104219]\n",
      " [-1.21465366]\n",
      " [-0.64624879]\n",
      " [ 0.08494682]\n",
      " [ 0.21045274]\n",
      " [ 0.55772969]\n",
      " [-1.98406389]\n",
      " [-1.98406389]\n",
      " [ 1.06192913]\n",
      " [ 0.66518692]\n",
      " [ 1.15401755]\n",
      " [-1.00950893]\n",
      " [-0.82104219]\n",
      " [ 0.96720905]\n",
      " [-1.21465366]\n",
      " [ 1.49928611]\n",
      " [-0.48290531]\n",
      " [-0.82104219]\n",
      " [ 0.44637808]\n",
      " [ 1.15401755]\n",
      " [ 0.55772969]\n",
      " [-0.82104219]\n",
      " [ 1.49928611]\n",
      " [-0.04635033]\n",
      " [ 0.66518692]\n",
      " [-1.44069733]\n",
      " [-0.1841502 ]\n",
      " [-0.04635033]\n",
      " [ 1.15401755]\n",
      " [ 1.65991961]\n",
      " [-1.69387693]\n",
      " [-0.32931499]\n",
      " [-0.32931499]\n",
      " [ 0.96720905]]\n",
      "(1635, 2500, 12) (1635, 1) (234, 2500, 12) (234, 1) (468, 2500, 12) (468, 1) (0,) (0,) (0,)\n"
     ]
    }
   ],
   "source": [
    "og_pt, train_labels, x_train, x_val, x_test, y_train, y_val, y_test, dems_train, dems_val, dems_test, train_labels_cat, val_labels_cat, test_labels_cat = load_data('sean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class ValidDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, ecgs, labels, dems=None, batch_size=32, dim=(2500), n_channels=12,\n",
    "                 n_classes=60, shuffle=True, train_cats = None):\n",
    "        'Initialization'\n",
    "        self.ndim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.ecgs = ecgs\n",
    "        self.ecg_inds = np.arange(ecgs.shape[0])\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.train_cats = train_cats\n",
    "        self.dems = dems\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.ecgs.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.ecg_inds[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.ecg_inds = np.arange(self.ecgs.shape[0])\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.ecg_inds)\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "#         # Initialization\n",
    "#         X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "#         y = np.empty((self.batch_size, self.n_classes), dtype=float32)\n",
    "\n",
    "        X = self.ecgs[indexes,:,:]\n",
    "        y = self.labels[indexes, :]\n",
    "#         d = self.dems[indexes,:]\n",
    "\n",
    "#         return [np.array(X),np.array(d)], y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, ecgs, labels, dems=None, batch_size=32, dim=(2500), n_channels=12,\n",
    "                 n_classes=60, shuffle=True, train_cats = None):\n",
    "        'Initialization'\n",
    "        self.ndim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.ecgs = ecgs\n",
    "        self.ecg_inds = np.arange(ecgs.shape[0])\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.train_cats = train_cats\n",
    "        self.dems = dems\n",
    "        self.calc_sampling_weights()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(self.ecgs.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        \n",
    "        indexes = np.random.choice(self.ecg_inds, size=self.batch_size, replace=False, p=self.ind_weights)\n",
    "        \n",
    "#         indexes = self.ecg_inds[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "#         pdb.set_trace()\n",
    "        \n",
    "        X, y = self.__data_generation(indexes)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def calc_sampling_weights(self):\n",
    "        'Initially, calculates sampling weights for each training item'\n",
    "        \n",
    "        cats, cat_counts = np.unique(self.train_cats,return_counts=True)\n",
    "        weights = np.zeros(len(cats))\n",
    "        for xx in range(len(cats)):\n",
    "            weights[xx] = 1/ ( cat_counts[xx]/len(self.train_cats)  )\n",
    "        \n",
    "        tot_weight = sum([weights[xx] * cat_counts[xx] for xx in range(len(weights))])\n",
    "        scaled_weights = []\n",
    "        for weight in weights:\n",
    "            scaled_weights.append(weight/tot_weight)\n",
    "        \n",
    "        ind_weights = np.zeros(self.ecgs.shape[0])\n",
    "        for ii, val in enumerate(self.train_cats):\n",
    "            ind_weights[ii] = scaled_weights[val]\n",
    "        \n",
    "        print('Weights', weights)\n",
    "        print('Scaled_weights',scaled_weights)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.ind_weights = ind_weights\n",
    "\n",
    "\n",
    "    def __data_generation(self, indexes):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "#         # Initialization\n",
    "#         X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "#         y = np.empty((self.batch_size, self.n_classes), dtype=float32)\n",
    "\n",
    "        X = self.ecgs[indexes,:,:]\n",
    "        y = self.labels[indexes, :]\n",
    "        \n",
    "#         d = []\n",
    "#         for each in indexes:\n",
    "#             d.append(self.dems[each,:])\n",
    "#         d = self.dems[indexes,:]\n",
    "#         print(d.shape)\n",
    "        \n",
    "#         pdb.set_trace()\n",
    "        \n",
    "#         return [np.array(X),np.array(d)], y\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def build_unet(**params):\n",
    "    import keras\n",
    "    from keras.layers import Conv1D, BatchNormalization, Add, MaxPooling1D\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.layers.core import Dense, Activation, Flatten\n",
    "    from keras.models import Model\n",
    "    from keras.layers import Input\n",
    "    from keras.layers import Dropout\n",
    "    \n",
    "    \n",
    "    inputs = Input(shape=params['input_shape'],\n",
    "                   dtype='float32',\n",
    "                   name='inputs')\n",
    "    \n",
    "    r = lambda: keras.regularizers.l2(params[\"l2_kern_weight\"])\n",
    "\n",
    "    layer = Conv1D(\n",
    "             filters=params[\"conv_num_filters_start\"],\n",
    "             kernel_size=params[\"conv_filter_length\"],\n",
    "             strides=1,\n",
    "             padding='same', \n",
    "             kernel_regularizer=r(), \n",
    "             kernel_initializer=params[\"conv_init\"])(inputs)\n",
    "    \n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation(params[\"conv_activation\"])(layer)\n",
    "\n",
    "    layer2 = Conv1D(\n",
    "             filters=params[\"conv_num_filters_start\"],\n",
    "             kernel_size=params[\"conv_filter_length\"],\n",
    "             strides=1,\n",
    "             padding='same', \n",
    "             kernel_regularizer=r(), \n",
    "             kernel_initializer=params[\"conv_init\"])(layer)\n",
    "\n",
    "    layer2 = BatchNormalization()(layer2)\n",
    "    layer2 = Activation(params[\"conv_activation\"])(layer2)\n",
    "    layer2 = Dropout(params[\"conv_dropout\"])(layer2)\n",
    "    layer2 = Conv1D(\n",
    "                 filters=params[\"conv_num_filters_start\"],\n",
    "                 kernel_size=params[\"conv_filter_length\"],\n",
    "                 strides=1,\n",
    "                 padding='same', \n",
    "                 kernel_regularizer=r(), \n",
    "                 kernel_initializer=params[\"conv_init\"])(layer2)\n",
    "    \n",
    "    layer = Add()([layer,layer2])\n",
    "    layer = MaxPooling1D(pool_size=2, strides=2,padding='same')(layer)\n",
    "    \n",
    "    for i in range(1, 1 + params[\"num_middle_layers\"]):\n",
    "        layer2 = layer\n",
    "        n_filters = params[\"conv_num_filters_start\"] * 2 ** (i // params['conv_increase_channels_at'])\n",
    "\n",
    "        for j in range(params[\"num_convs_per_layer\"]):\n",
    "            layer2 = BatchNormalization()(layer2)\n",
    "            layer2 = Activation(params[\"conv_activation\"])(layer2)\n",
    "            layer2 = Dropout(params[\"conv_dropout\"])(layer2)\n",
    "            layer2 = Conv1D(\n",
    "                 filters=n_filters,\n",
    "                 kernel_size=params[\"conv_filter_length\"],\n",
    "                 strides=1,\n",
    "                 padding='same', \n",
    "                 kernel_regularizer=r(), \n",
    "                 kernel_initializer=params[\"conv_init\"])(layer2)\n",
    "            \n",
    "        if i % params['conv_increase_channels_at'] == 0:\n",
    "            layer = layer2\n",
    "        else:\n",
    "            layer = Add()([layer,layer2])\n",
    "\n",
    "        if i % params['conv_pool_at'] == 0:\n",
    "            layer = MaxPooling1D(pool_size=2, strides=2,padding='same')(layer)\n",
    "\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation(params[\"conv_activation\"])(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    for i in range(params[\"hidden_layers\"]):\n",
    "        if i == 0:\n",
    "            layer = Dense(params[\"hidden_size_1\"])(layer)\n",
    "        if i == 1:\n",
    "            layer = Dense(params[\"hidden_size_2\"])(layer)\n",
    "\n",
    "    layer = Dense(params[\"num_categories\"])(layer)\n",
    "    \n",
    "    output = Activation(\"linear\")(layer)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[output])\n",
    "\n",
    "    optimizer = Adam(\n",
    "        lr=params[\"learning_rate\"],\n",
    "        clipnorm=params.get(\"clipnorm\", 1))\n",
    "\n",
    "    model.compile(loss='mae',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_filename_for_saving(save_dir):\n",
    "    return os.path.join(save_dir,\n",
    "            \"{val_loss:.3f}-{epoch:03d}-{loss:.3f}.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights [ 1.06241873 17.02083333]\n",
      "Scaled_weights [0.00032509752925877764, 0.005208333333333333]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             (None, 2500, 12)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 2500, 64)     12352       inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 2500, 64)     256         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 2500, 64)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 2500, 64)     65600       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 2500, 64)     256         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 2500, 64)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2500, 64)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 2500, 64)     65600       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 2500, 64)     0           activation_1[0][0]               \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1250, 64)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1250, 64)     256         max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1250, 64)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1250, 64)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 1250, 64)     65600       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1250, 64)     0           max_pooling1d_1[0][0]            \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1250, 64)     256         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1250, 64)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1250, 64)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 1250, 64)     65600       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1250, 64)     0           add_2[0][0]                      \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 625, 64)      0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 625, 64)      256         max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 625, 64)      0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 625, 64)      0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 625, 64)      65600       dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 625, 64)      0           max_pooling1d_2[0][0]            \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 625, 64)      256         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 625, 64)      0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 625, 64)      0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 625, 128)     131200      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 313, 128)     0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 313, 128)     512         max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 313, 128)     0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 313, 128)     0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 313, 128)     262272      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 313, 128)     0           max_pooling1d_3[0][0]            \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 313, 128)     512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 313, 128)     0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 313, 128)     0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 313, 128)     262272      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 313, 128)     0           add_5[0][0]                      \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 157, 128)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 157, 128)     512         max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 157, 128)     0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 157, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 157, 128)     262272      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 157, 128)     0           max_pooling1d_4[0][0]            \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 157, 128)     512         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 157, 128)     0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 20096)        0           activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         20579328    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            1025        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 1)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 21,842,305\n",
      "Trainable params: 21,840,513\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "params={\n",
    "    \"conv_subsample_lengths\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], #12 layers\n",
    "#     \"conv_subsample_lengths\": [1, 2, 1, 2, 1, 2, 1, 2], #8 layers\n",
    "#     \"conv_subsample_lengths\": [1, 2, 1, 2, 1, 2], #6 layers\n",
    "#     \"conv_subsample_lengths\": [1, 2, 1, 2, 1], #5 layers\n",
    "    \"conv_filter_length\": 16,\n",
    "    \"conv_num_filters_start\": 64,\n",
    "    \"conv_init\": \"he_normal\",\n",
    "    \"conv_activation\": \"relu\",\n",
    "    \"conv_dropout\": 0.2,\n",
    "    \"conv_num_skip\": 2,\n",
    "    \"conv_increase_channels_at\": 4,\n",
    "    \"conv_pool_at\": 2,\n",
    "    \"num_categories\":1,\n",
    "    \n",
    "    \"input_shape\":[2500,12],\n",
    "    \n",
    "    \"l2_kern_weight\": 0.0001,\n",
    "    \"l2_bias_weight\": 0.0001,\n",
    "\n",
    "    \"hidden_layers\": 1,\n",
    "    \"hidden_size_1\":1024,\n",
    "#     \"hidden_size_2\":256,\n",
    "    \n",
    "    \n",
    "    \"num_middle_layers\": 7,\n",
    "    \"num_convs_per_layer\": 1,\n",
    "    \n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"batch_size\": 128,\n",
    "    \n",
    "    \"compile\": True,\n",
    "    \n",
    "}\n",
    "# assert len(params['conv_subsample_lengths']) == params[\"conv_filter_length\"]\n",
    "\n",
    "gen_params = {\n",
    "    'dim': (2500),\n",
    "    'batch_size': 128,\n",
    "    'n_classes': 1,\n",
    "    'n_channels': 12,\n",
    "    'shuffle': True,\n",
    "    'train_cats': np.array(train_labels_cat).squeeze()\n",
    "}\n",
    "\n",
    "# # Generators\n",
    "training_generator = TrainDataGenerator(x_train[:,:2500,:], y_train, **gen_params)\n",
    "validation_generator = ValidDataGenerator(x_val[:,:2500,:], y_val, **gen_params)\n",
    "\n",
    "\n",
    "model = build_unet(**params)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1635 samples, validate on 234 samples\n",
      "Epoch 1/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 6.8870 - mean_absolute_error: 6.7002 - val_loss: 1.2603 - val_mean_absolute_error: 1.0723\n",
      "Epoch 2/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 4.5464 - mean_absolute_error: 4.3576 - val_loss: 1.8155 - val_mean_absolute_error: 1.6256\n",
      "Epoch 3/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 2.7284 - mean_absolute_error: 2.5380 - val_loss: 1.0355 - val_mean_absolute_error: 0.8445\n",
      "Epoch 4/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 1.5561 - mean_absolute_error: 1.3647 - val_loss: 0.9985 - val_mean_absolute_error: 0.8068\n",
      "Epoch 5/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 1.0993 - mean_absolute_error: 0.9077 - val_loss: 0.9970 - val_mean_absolute_error: 0.8059\n",
      "Epoch 6/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 1.0179 - mean_absolute_error: 0.8273 - val_loss: 0.9969 - val_mean_absolute_error: 0.8071\n",
      "Epoch 7/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 1.0124 - mean_absolute_error: 0.8232 - val_loss: 0.9805 - val_mean_absolute_error: 0.7921\n",
      "Epoch 8/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 1.0312 - mean_absolute_error: 0.8434 - val_loss: 0.9749 - val_mean_absolute_error: 0.7879\n",
      "Epoch 9/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9978 - mean_absolute_error: 0.8113 - val_loss: 0.9694 - val_mean_absolute_error: 0.7836\n",
      "Epoch 10/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9744 - mean_absolute_error: 0.7892 - val_loss: 0.9582 - val_mean_absolute_error: 0.7737\n",
      "Epoch 11/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9563 - mean_absolute_error: 0.7727 - val_loss: 0.9745 - val_mean_absolute_error: 0.7920\n",
      "Epoch 12/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9534 - mean_absolute_error: 0.7717 - val_loss: 0.9555 - val_mean_absolute_error: 0.7750\n",
      "Epoch 13/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9509 - mean_absolute_error: 0.7714 - val_loss: 0.9508 - val_mean_absolute_error: 0.7723\n",
      "Epoch 14/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9477 - mean_absolute_error: 0.7701 - val_loss: 0.9356 - val_mean_absolute_error: 0.7590\n",
      "Epoch 15/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9357 - mean_absolute_error: 0.7600 - val_loss: 0.9468 - val_mean_absolute_error: 0.7722\n",
      "Epoch 16/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9457 - mean_absolute_error: 0.7720 - val_loss: 0.9249 - val_mean_absolute_error: 0.7524\n",
      "Epoch 17/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9210 - mean_absolute_error: 0.7495 - val_loss: 0.9434 - val_mean_absolute_error: 0.7730\n",
      "Epoch 18/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9438 - mean_absolute_error: 0.7741 - val_loss: 0.9842 - val_mean_absolute_error: 0.8154\n",
      "Epoch 19/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9334 - mean_absolute_error: 0.7653 - val_loss: 0.9014 - val_mean_absolute_error: 0.7340\n",
      "Epoch 20/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9529 - mean_absolute_error: 0.7860 - val_loss: 0.9419 - val_mean_absolute_error: 0.7756\n",
      "Epoch 21/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9003 - mean_absolute_error: 0.7345 - val_loss: 0.9626 - val_mean_absolute_error: 0.7976\n",
      "Epoch 22/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.9171 - mean_absolute_error: 0.7527 - val_loss: 0.9496 - val_mean_absolute_error: 0.7861\n",
      "Epoch 23/50\n",
      "1635/1635 [==============================] - 3s 2ms/step - loss: 0.8773 - mean_absolute_error: 0.7147 - val_loss: 0.9042 - val_mean_absolute_error: 0.7427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f049f1a5358>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stopping = keras.callbacks.EarlyStopping(patience=4)\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    factor=0.1,\n",
    "    patience=4,\n",
    "    min_lr=1e-4 * 0.001)\n",
    "\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(filepath=get_filename_for_saving('/volume/ECG/pcw_regression/run1_results'),\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(x_train[:,:2500,:], y_train, batch_size=128,\n",
    "                    validation_data= (x_val[:,:2500,:], y_val),\n",
    "                    epochs=50,\n",
    "                    callbacks=[checkpointer, reduce_lr, stopping], shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.901-019-0.933.hdf5  0.958-010-0.974.hdf5  0.997-006-1.018.hdf5\r\n",
      "0.925-016-0.946.hdf5  0.969-009-0.998.hdf5  0.999-004-1.556.hdf5\r\n",
      "0.936-014-0.948.hdf5  0.975-008-1.031.hdf5  1.036-003-2.728.hdf5\r\n",
      "0.951-013-0.951.hdf5  0.980-007-1.012.hdf5  1.260-001-6.887.hdf5\r\n",
      "0.955-012-0.953.hdf5  0.997-005-1.099.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "ls '/volume/ECG/pcw_regression/run1_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in specific saved model and test performance\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score \n",
    "\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "# Best multi class one with rnn+attn: '1.081-009-1.113.hdf5'\n",
    "model_1 =  keras.models.load_model('/volume/ECG/pcw_regression/run1_results/0.901-019-0.933.hdf5', custom_objects={'root_mean_squared_error':root_mean_squared_error, 'f1_score': f1_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model_1.predict(x_test[:,:2500,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468, 1) (468, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reverse transformations and scaling made during data loading\n",
    "pt = PowerTransformer()\n",
    "pt.fit(train_labels)\n",
    "y_test_adj = pt.inverse_transform(y_test)\n",
    "y_predicted_adj = pt.inverse_transform(y_predicted)\n",
    "\n",
    "print(y_test_adj.shape, y_predicted_adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compare adjusted predictions to test \n",
    "#for each in np.concatenate( ((y_predicted_adj).reshape(-1,1),(y_test_adj).reshape(-1,1) ), axis=1):\n",
    "#    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2502.7139797210693"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi = np.abs(y_predicted_adj-y_test_adj).ravel()\n",
    "hi = pd.Series(hi).sum()\n",
    "hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.347679443848439"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAE\n",
    "hi/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    468.000000\n",
      "mean      14.282051\n",
      "std        7.089028\n",
      "min        1.000000\n",
      "25%        9.000000\n",
      "50%       13.000000\n",
      "75%       19.000000\n",
      "max       38.000000\n",
      "Name: temp, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f01f054f2b0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize distribution of the test set\n",
    "mdf = pd.DataFrame()\n",
    "mdf['temp'] = np.array(y_test_adj).squeeze()\n",
    "print(mdf['temp'].describe())\n",
    "mdf['temp'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    468.000000\n",
      "mean      13.504971\n",
      "std        3.573232\n",
      "min        4.080896\n",
      "25%       10.654010\n",
      "50%       12.674493\n",
      "75%       15.873813\n",
      "max       35.844547\n",
      "Name: temp, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f03982f13c8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize distribution of predictions\n",
    "mdf = pd.DataFrame()\n",
    "mdf['temp'] = np.array(y_predicted_adj).squeeze()\n",
    "print(mdf['temp'].describe())\n",
    "mdf['temp'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.777081672961895"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check regression\n",
    "# MAE\n",
    "\n",
    "\n",
    "# MSE\n",
    "\n",
    "\n",
    "# R2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check performance of regression in bins \n",
    "#FOR DE-SCALED VALUES\n",
    "CUTOFFS = [-900000000000000, 17, 9000000000000000]\n",
    "y_test_categorized = []\n",
    "y_predicted_categorized = []\n",
    "\n",
    "df = pd.DataFrame(y_predicted_adj)\n",
    "df[df.isna().any(axis=1)] = 20\n",
    "\n",
    "for i in range(len(y_test_adj)):\n",
    "    for ii in range(1,len(CUTOFFS)):\n",
    "        if y_test_adj[i] >= CUTOFFS[ii-1] and y_test_adj[i] < CUTOFFS[ii]:\n",
    "            this_label = [0 for x in range(1,len(CUTOFFS))]\n",
    "            this_label[ii-1] = 1\n",
    "            y_test_categorized.append(this_label)\n",
    "        if y_predicted_adj[i][0] >= CUTOFFS[ii-1] and y_predicted_adj[i][0] < CUTOFFS[ii]:\n",
    "            this_label = [0 for x in range(1,len(CUTOFFS))]\n",
    "            this_label[ii-1] = 1\n",
    "            y_predicted_categorized.append(this_label)\n",
    "y_test_categorized = np.array(y_test_categorized)\n",
    "y_predicted_categorized = np.array(y_predicted_categorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.86      0.77       312\n",
      "           1       0.49      0.28      0.36       156\n",
      "\n",
      "   micro avg       0.66      0.66      0.66       468\n",
      "   macro avg       0.60      0.57      0.57       468\n",
      "weighted avg       0.63      0.66      0.63       468\n",
      "\n",
      "[[267  45]\n",
      " [112  44]]\n",
      "accuracy: 0.6645299145299145\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# for x in range(3):\n",
    "#     this_s = f1_score(y_test_categorized[:,x], y_predicted_categorized[:,x], pos_label=1)\n",
    "#     print(this_s)\n",
    "    \n",
    "Y_test = np.argmax(y_test_categorized, axis=1)\n",
    "Y_predicted = np.argmax(y_predicted_categorized, axis=1)\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "print(classification_report(Y_test, Y_predicted))\n",
    "print(confusion_matrix(Y_test, Y_predicted))\n",
    "print('accuracy: {}'.format(accuracy_score(Y_test, Y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-43-35ba2386ea85>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-43-35ba2386ea85>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print('test' + 2.asstr)\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
