{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'model_zoo.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first initial proposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_models_1D.tfkeras import Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "def step_color(bloc_order, position):\n",
    "    formattedText = []\n",
    "    for pos, i in enumerate(bloc_order):\n",
    "        pos += 1\n",
    "        if pos != position and pos != len(bloc_order):\n",
    "            formattedText.append('{} -> '.format(i))\n",
    "        elif pos == position:\n",
    "            formattedText.append(colored('{}'.format(i),'white','on_red'))\n",
    "            formattedText.append(' -> ')\n",
    "\n",
    "        else:\n",
    "            formattedText.append(i)\n",
    "\n",
    "    return ''.join(formattedText)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wandb(dict_yaml, step='model'):\n",
    "    \n",
    "    if step == 'model':\n",
    "        config={\n",
    "            'name': dict_yaml['name_sweep'],\n",
    "            'method': dict_yaml['method'],\n",
    "            'metric': {'goal': dict_yaml['metric']['goal'], 'name': dict_yaml['metric']['name']},\n",
    "            'parameters':\n",
    "            {\n",
    "                'models': {'values':dict_yaml['config']['models']}\n",
    "\n",
    "            }\n",
    "        }    \n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score\n",
    "\n",
    "def macro_auc(y_true, y_pred):\n",
    "    return roc_auc_score(y_true, y_pred, multi_class='ovr',average='macro')\n",
    "\n",
    "def micro_auc(y_true, y_pred):\n",
    "    return roc_auc_score(y_true, y_pred, multi_class='ovr', average='micro')\n",
    "\n",
    "def sample_auc(y_true, y_pred):\n",
    "    return roc_auc_score(y_true, y_pred,  multi_class='ovr' , average='samples')\n",
    "\n",
    "def averaged_metric(y_true, y_pred):\n",
    "    return (micro_auc(y_true, y_pred) + macro_auc(y_true, y_pred))/2\n",
    "\n",
    "\n",
    "def auc_pr(y_true, y_pred):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "    return roc_auc_score(recall, precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mCurently running core model optimization\u001b[0m\n",
      "\u001b[32mJob name :\u001b[0m \u001b[36mtrain run 1\u001b[0m\n",
      "\u001b[32mSweep name :\u001b[0m \u001b[36msweep 1\u001b[0m\n",
      "\u001b[32mOptimization method :\u001b[0m \u001b[36mbayes\u001b[0m\n",
      "\u001b[32mOptimization applied to the metric :\u001b[0m \u001b[36maveraged_metric\u001b[0m\n",
      "\u001b[32mOptimization goal :\u001b[0m \u001b[36mmaximize\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[32mChosen parameters are:\u001b[0m\n",
      "{'allow_ensemble': True,\n",
      " 'best_to_select': 3,\n",
      " 'bloc_order': ['model', 'optimizer', 'loss', 'regularization', 'fine_tuning'],\n",
      " 'block_to_run': 1,\n",
      " 'config': {'kernel_size': ['default', 5, 7, 9],\n",
      "            'models': ['resnet18',\n",
      "                       'resnet34',\n",
      "                       'resnet50',\n",
      "                       'resnet101',\n",
      "                       'seresnet18',\n",
      "                       'seresnet34',\n",
      "                       'seresnet50',\n",
      "                       'seresnet101',\n",
      "                       'seresnext50',\n",
      "                       'seresnext101',\n",
      "                       'resnext50',\n",
      "                       'resnext101',\n",
      "                       'densenet121',\n",
      "                       'densenet169',\n",
      "                       'mobilenetv2',\n",
      "                       'inceptionresnetv2',\n",
      "                       'inceptionv3',\n",
      "                       'EfficientNetB0',\n",
      "                       'EfficientNetB1',\n",
      "                       'EfficientNetB2',\n",
      "                       'EfficientNetB3',\n",
      "                       'EfficientNetB4',\n",
      "                       'EfficientNetB5',\n",
      "                       'EfficientNetB6',\n",
      "                       'EfficientNetB7',\n",
      "                       'EfficientNetV2B0',\n",
      "                       'EfficientNetV2B1',\n",
      "                       'EfficientNetV2B2',\n",
      "                       'EfficientNetV2B3',\n",
      "                       'EfficientNetV2S',\n",
      "                       'EfficientNetV2M',\n",
      "                       'EfficientNetV2L',\n",
      "                       'EfficientNetB0_spectre',\n",
      "                       'EfficientNetB1_spectre',\n",
      "                       'EfficientNetB2_spectre',\n",
      "                       'EfficientNetB3_spectre',\n",
      "                       'EfficientNetB4_spectre',\n",
      "                       'EfficientNetB5_spectre',\n",
      "                       'EfficientNetB6_spectre',\n",
      "                       'EfficientNetB6_spectre'],\n",
      "            'stride_size': ['default', 2, 3, 5, 7]},\n",
      " 'loss': ['BCE',\n",
      "          {'BinaryFocalLoss': [{...}]},\n",
      "          'BinaryPolyLoss',\n",
      "          'LabelSmoothingCrossEntropy'],\n",
      " 'method': 'bayes',\n",
      " 'metric': {'goal': 'maximize', 'name': 'averaged_metric'},\n",
      " 'name_sweep': 'sweep 1',\n",
      " 'optimization': {'batch_size': [64, 128, 256],\n",
      "                  'learning_rate': ['1e-6', '1e-1'],\n",
      "                  'optimizer': ['Adam', 'RMSProp', 'SGD']},\n",
      " 'reg_approaches': [{'Dropout': [0, 0.1, 0.2, 0.3, 0.4, 0.5]},\n",
      "                    {'L1': ['1e-3', '1e-6']},\n",
      "                    {'L2': ['1e-3', '1e-6']},\n",
      "                    'Labelsmoothing'],\n",
      " 'resume_previous_run': False,\n",
      " 'run_name': 'train run 1',\n",
      " 'save_dir': '/volume/core_model',\n",
      " 'save_info': True,\n",
      " 'verbose': True}\n",
      "\n",
      "\n",
      "\u001b[32mCurrently running:\u001b[0m\n",
      "\u001b[41m\u001b[37mmodel\u001b[0m -> optimizer -> loss -> regularization -> fine_tuning\n",
      "\n",
      "\n",
      "\u001b[32mStep 1.1:\u001b[0m\n",
      "Main backbone\u001b[0m\n",
      "{'name': 'sweep 1', 'method': 'bayes', 'metric': {'goal': 'maximize', 'name': 'averaged_metric'}, 'parameters': {'models': {'values': ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'seresnet18', 'seresnet34', 'seresnet50', 'seresnet101', 'seresnext50', 'seresnext101', 'resnext50', 'resnext101', 'densenet121', 'densenet169', 'mobilenetv2', 'inceptionresnetv2', 'inceptionv3', 'EfficientNetB0', 'EfficientNetB1', 'EfficientNetB2', 'EfficientNetB3', 'EfficientNetB4', 'EfficientNetB5', 'EfficientNetB6', 'EfficientNetB7', 'EfficientNetV2B0', 'EfficientNetV2B1', 'EfficientNetV2B2', 'EfficientNetV2B3', 'EfficientNetV2S', 'EfficientNetV2M', 'EfficientNetV2L', 'EfficientNetB0_spectre', 'EfficientNetB1_spectre', 'EfficientNetB2_spectre', 'EfficientNetB3_spectre', 'EfficientNetB4_spectre', 'EfficientNetB5_spectre', 'EfficientNetB6_spectre', 'EfficientNetB6_spectre']}}}\n",
      "Create sweep with ID: uny81555\n",
      "Sweep URL: https://app.wandb.ai/anolin/train%20run%201/sweeps/uny81555\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=4)\n",
    "\n",
    "from classification_models_1D.tfkeras import Classifiers\n",
    "from termcolor import colored\n",
    "import yaml\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from statistics import mean, stdev \n",
    "import pickle\n",
    "\n",
    "data_dir = '/volume/core_model/data_split/'\n",
    "\n",
    "with open('test.yml', 'r') as file:\n",
    "    prime_service = yaml.safe_load(file)\n",
    "prime_service\n",
    "\n",
    "#print user paraeters\n",
    "\n",
    "print(colored('Curently running core model optimization', 'green'))\n",
    "print(colored('Job name :', 'green'), colored(prime_service['run_name'], 'cyan'))\n",
    "print(colored('Sweep name :', 'green'), colored(prime_service['name_sweep'], 'cyan'))\n",
    "print(colored('Optimization method :', 'green'), colored(prime_service['method'], 'cyan'))\n",
    "print(colored('Optimization applied to the metric :', 'green'), colored(prime_service['metric']['name'], 'cyan'))\n",
    "print(colored('Optimization goal :', 'green'), colored(prime_service['metric']['goal'], 'cyan'))\n",
    "print('\\n')\n",
    "print(colored('Chosen parameters are:', 'green'))\n",
    "pp.pprint(prime_service)\n",
    "\n",
    "block_counter = 1\n",
    "print('\\n')\n",
    "print(colored('Currently running:','green'))\n",
    "print(step_color(prime_service['bloc_order'], block_counter))\n",
    "print('\\n')\n",
    "print(colored('Step {}.1:'.format(block_counter),'green'))\n",
    "print(colored('Main backbone'))\n",
    "\n",
    "# select a model to run\n",
    "sweep_configuration = init_wandb(prime_service, step='model')\n",
    "print(sweep_configuration)\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=prime_service['run_name'])\n",
    "\n",
    "\n",
    "# first 1.1 block\n",
    "#wandb.run=None\n",
    "\n",
    "def block_1():\n",
    "    print(\"adadadw\")\n",
    "    run = wandb.init()\n",
    "    print(\"oooo\")\n",
    "\n",
    "    save_dict = {}\n",
    "\n",
    "    #load model\n",
    "    print(wandb.config)\n",
    "    loaded_architecture, preprocess_input = Classifiers.get(wandb.config.models)\n",
    "\n",
    "    print('Currently running {}'.format(wandb.config.models))\n",
    "\n",
    "    model_selected = loaded_architecture(\n",
    "    input_shape=(2500, 12),\n",
    "    weights=None\n",
    "    )\n",
    "\n",
    "    optimizer_ = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    loss_func = 'binary_crossentropy'\n",
    "\n",
    "    model_selected.compile(optimizer=optimizer_, loss=loss_func, metrics=['accuracy',macro_auc,micro_auc,sample_auc,averaged_metric])     \n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=2)\n",
    "    lrplateau = ReduceLROnPlateau(  \n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    mode=\"auto\", \n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=1e-12)\n",
    "\n",
    "    #load data\n",
    "\n",
    "    list_scores = list()\n",
    "    for split in range(3):\n",
    "\n",
    "        curr_data_dir = os.path.join(data_dir,'split_{}'.format(split))\n",
    "        X_train = np.load(os.path.join(curr_data_dir,'train_X.npy')) \n",
    "        Y_train = np.load(os.path.join(curr_data_dir,'train_Y.npy')) \n",
    "\n",
    "        X_val = np.load(os.path.join(curr_data_dir,'val_X.npy')) \n",
    "        Y_val = np.load(os.path.join(curr_data_dir,'val_Y.npy')) \n",
    "\n",
    "        history = model_selected.fit(X_train, Y_train, verbose=1,batch_size=64, epochs=250, callbacks=[early_stop,lrplateau], validation_data=(X_val,Y_val)) \n",
    "\n",
    "        #evaluate model\n",
    "        X_test = np.load(os.path.join(curr_data_dir,'val_X.npy')) \n",
    "        Y_test = np.load(os.path.join(curr_data_dir,'val_Y.npy')) \n",
    "        Y_test_pred = model_selected.predict(X_test)\n",
    "\n",
    "        dict_results = dict()\n",
    "        for i in [macro_auc,micro_auc,sample_auc,averaged_metric,auc_pr]:\n",
    "            dict_results.update({i.__name__:i(Y_test,Y_test_pred)})\n",
    "\n",
    "        list_scores.append(averaged_metric(Y_test,Y_test_pred))\n",
    "\n",
    "        if prime_service['save_info'] == True:\n",
    "            save_dict.update({'{}_{}'.format(wandb.config.models,split):{'history':history.history, 'test_performances':dict_results}})\n",
    "\n",
    "    print('Perfroances for {} were {} Â± {}'.format(wandb.config.models,mean(list_scores), stdev(list_scores)))\n",
    "\n",
    "\n",
    "    wandb.log({\n",
    "    'model': wandb.config.models, \n",
    "    'macro_auc': dict_results['macro_auc'],\n",
    "    'micro_auc': dict_results['micro_auc'],\n",
    "    'sample_auc': dict_results['sample_auc'],\n",
    "    'averaged_metric': dict_results['averaged_metric'],\n",
    "    'auc_pr':dict_results['auc_pr']\n",
    "    })\n",
    "\n",
    "    if prime_service['bloc_order'] == True:\n",
    "        with open(os.path.join(prime_service['save_dir'],'saved_outputs_detailed.{}.pickle'.format(prime_service['name_sweep'])), 'wb') as handle:\n",
    "            pickle.dump(save_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb: Agent Starting Run: cnh7e2dd with config:\n",
      "\tmodels: EfficientNetV2B0\n",
      "wandb: Agent Started Run: cnh7e2dd\n",
      "adadadw\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/anolin/train%20run%201\" target=\"_blank\">https://app.wandb.ai/anolin/train%20run%201</a><br/>\n",
       "                Sweep page: <a href=\"https://app.wandb.ai/anolin/train%20run%201/sweeps/uny81555\" target=\"_blank\">https://app.wandb.ai/anolin/train%20run%201/sweeps/uny81555</a><br/>\n",
       "Run page: <a href=\"https://app.wandb.ai/anolin/train%20run%201/runs/cnh7e2dd\" target=\"_blank\">https://app.wandb.ai/anolin/train%20run%201/runs/cnh7e2dd</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.15.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oooo\n",
      "wandb_version: 1\n",
      "\n",
      "_wandb:\n",
      "  desc: null\n",
      "  value:\n",
      "    cli_version: 0.9.7\n",
      "    framework: keras\n",
      "    is_jupyter_run: true\n",
      "    is_kaggle_kernel: false\n",
      "    python_version: 3.8.5\n",
      "models:\n",
      "  desc: null\n",
      "  value: EfficientNetV2B0\n",
      "\n",
      "Currently running EfficientNetV2B0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/wandb/wandb_agent.py\", line 64, in _start\n",
      "    function()\n",
      "  File \"<ipython-input-7-67fb3e3d3006>\", line 89, in main\n",
      "    X_train = np.load(os.path.join(curr_data_dir,'train_X.npy'))\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\", line 405, in load\n",
      "    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/volume/core_model/data_split/split_0/train_X.npy'\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=main, count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Exception('problem')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Exception('problem')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
