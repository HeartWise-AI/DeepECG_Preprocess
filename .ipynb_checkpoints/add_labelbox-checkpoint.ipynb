{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('max_colwidth', 200)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "\n",
    "def df_stats(df):\n",
    "    from tabulate import tabulate\n",
    "    print(\"\\n***** Shape: \", df.shape,\" *****\\n\")\n",
    "    \n",
    "    columns_list = df.columns.values.tolist()\n",
    "    isnull_list = df.isnull().sum().values.tolist()\n",
    "    isunique_list = df.nunique().values.tolist()\n",
    "    dtypes_list = df.dtypes.tolist()\n",
    "    \n",
    "    list_stat_val = list(zip(columns_list, isnull_list, isunique_list, dtypes_list))\n",
    "    df_stat_val = pd.DataFrame(list_stat_val, columns=['Name', 'Null', 'Unique', 'Dtypes'])\n",
    "    print(tabulate(df_stat_val, headers='keys', tablefmt='psql'))\n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the labelbox annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import labelbox\n",
    "# Enter your Labelbox API key here\n",
    "LB_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1c2VySWQiOiJjbDh2eGdsaHAweTlzMDh6ZmdyOXM3Z28zIiwib3JnYW5pemF0aW9uSWQiOiJjbDh2eGdsaDgweTlyMDh6ZjNybzNkbXg2IiwiYXBpS2V5SWQiOiJjbGV2aTA4ejEwMzBmMDczZjIzb2UxZnExIiwic2VjcmV0IjoiNTExMDIwNzcxODk3MmRjM2MyMTI0MDRjNTI5ZGFjY2UiLCJpYXQiOjE2NzgwMjcwOTQsImV4cCI6MjMwOTE3OTA5NH0.kTAKOI5Sm7wE3IeEjZTGpwc1u4aU0Ya5mzt4eOHa-wQ\"\n",
    "# Create Labelbox client\n",
    "client = labelbox.Client(api_key=LB_API_KEY)\n",
    "PROJECT_ID = 'cl8vxju2k0z0q07ztfyt5dr7l'\n",
    "project = client.get_project(PROJECT_ID)\n",
    "labels = project.export_v2(params={\n",
    "\t\"data_row_details\": False,\n",
    "\t\"metadata_fields\": False,\n",
    "\t\"attachments\": False,\n",
    "\t\"project_details\": False,\n",
    "\t\"performance_details\": False,\n",
    "\t\"label_details\": False,\n",
    "\t\"interpolated_frames\": False\n",
    "  })\n",
    "labels.wait_till_done()\n",
    "\n",
    "if labels.errors:\n",
    "  print(labels.errors)\n",
    "\n",
    "export_json = labels.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the previous parquet and make sure that the patient id has a zero-padded length of 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dir_ = '/media/data1/ravram/DeepECG/ekg_waveforms_output/df_xml_2023_03_30_n_1633856_with_labelbox.parquet'\n",
    "df_ = pd.read_parquet(dir_, engine='fastparquet')\n",
    "\n",
    "#make sure patient len(id) == 7\n",
    "df_['RestingECG_PatientDemographics_PatientID'] = [n.zfill(7) for n in df_['RestingECG_PatientDemographics_PatientID'].tolist()]\n",
    "display(len(df_))  \n",
    "\n",
    "### Groupby xml_path\n",
    "df_group = df_.groupby('xml_path').agg(lambda x: x.tolist())\n",
    "display(len(df_group))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the annotated ECG csv and similarly z-pad patient id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"/media/data1/anolin/20221002_ECG_mod_diagnosis_sampled_3600.csv\")\n",
    "new_df['patientid'] = [str(n).zfill(7) for n in new_df['patientid'].tolist()]\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this basically fills a dict where the key is a unique diag and the values\n",
    "# is a list that is filled with each patient ID with an exact match for that diag\n",
    "dict_diag = dict()\n",
    "#ROBERT : 2023-11-22 CHANGED TO DIAGNOSIS\n",
    "for k,v in zip(new_df['Diagnosis'].tolist(),new_df['patientid'].tolist()):\n",
    "    if k in dict_diag:\n",
    "        dict_diag[k].append(v)\n",
    "\n",
    "    else:\n",
    "        dict_diag.update({k:[v]})\n",
    "\n",
    "#dict_diag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the labelbox hot-encoding for the Labelbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_labels(label_dict):\n",
    "    \"\"\"\n",
    "    Flattens the labels from a Labelbox sub-dictionary into a predictable structure.\n",
    "\n",
    "    Parameters:\n",
    "        label_dict (dict): A Labelbox sub-dictionary object.\n",
    "\n",
    "    Returns:\n",
    "        dict: Flattened dictionary with key categories and their corresponding features.\n",
    "    \"\"\"\n",
    "    flattened_dict = {\n",
    "        'Rhythm': [], 'QRS complex': [], 'Wave criterias': [], \n",
    "        'Conduction': [], 'Chamber enlargement': [], 'Other': [],\n",
    "        'ST segments': [], 'P-wave morphology': []\n",
    "    }\n",
    "    classification_dict = label_dict['classifications']\n",
    "\n",
    "    for category in classification_dict:\n",
    "        category_name = category['name']\n",
    "        for feature in category['checklist_answers']:\n",
    "            flattened_dict[category_name].append(feature['name'])\n",
    "\n",
    "    return flattened_dict\n",
    "\n",
    "\n",
    "def adjust_name(label_dict):\n",
    "    \"\"\"\n",
    "    Modifies the dictionary to use the original patient ID as the key.\n",
    "    \n",
    "    Parameters:\n",
    "        label_dict (dict): Dictionary with Labelbox original ECG ID as key.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with Patient ID as the new key.\n",
    "    \"\"\"\n",
    "    modified_dict = {}\n",
    "    for key, value in label_dict.items():\n",
    "        modified_key = key.split('_')[1]\n",
    "        modified_dict[modified_key] = value\n",
    "\n",
    "    return modified_dict\n",
    "\n",
    "\n",
    "def get_single_value(label_dict):\n",
    "    \"\"\"\n",
    "    Filters the dictionary to include only non-empty label categories.\n",
    "    \n",
    "    Parameters:\n",
    "        label_dict (dict): Dictionary with both empty and filled Labelbox label categories.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with only positive labels, without category information.\n",
    "    \"\"\"\n",
    "\n",
    "    filtered_dict = {}\n",
    "\n",
    "    for key, value in label_dict.items():\n",
    "        for subkey, subvalue in value['annotations'].items():\n",
    "            if key not in filtered_dict:\n",
    "                filtered_dict[key] = subvalue\n",
    "            else:\n",
    "                filtered_dict[key].extend(subvalue)\n",
    "\n",
    "    return filtered_dict\n",
    "\n",
    "\n",
    "def get_hot_labels(dict_):\n",
    "    \"\"\"\n",
    "    Generates a one_hot encoded_vector for a dictionnary containing its unique positive labels\n",
    "    it looks through the entire dictionnary creates a unique list of all the potnetial labels\n",
    "    and uses that list to, within another loop, generate the vector\n",
    "    \n",
    "    Parameters:\n",
    "        dict: a dictionnary with only positive labels no category\n",
    "    Returns:\n",
    "        dict: a dictionnary with patient id: one hot encoded vector\n",
    "        list: list of the features used, used to generate the df columns\n",
    "    \"\"\"\n",
    "\n",
    "    list_features = list()\n",
    "\n",
    "    for k,v in dict_.items():\n",
    "        for i in v:\n",
    "            if i not in list_features:\n",
    "                list_features.append(i)\n",
    "\n",
    "    out_dict = dict()\n",
    "    for k,v in dict_.items():\n",
    "        out_list = [0] * len(list_features)\n",
    "        for i in v:\n",
    "            for pos,ii in enumerate(list_features):\n",
    "                if ii == i:\n",
    "                    out_list[pos] = 1\n",
    "\n",
    "        out_dict.update({k:out_list})\n",
    "\n",
    "    return out_dict,list_features\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def generate_cleaned_dict_v2(label_box_dict):\n",
    "    \"\"\"\n",
    "    Finds and filteres the labelbox raw dict to extract only the labels and/or flatten it\n",
    "\n",
    "    Parameters:\n",
    "        dict: a labelbox object \n",
    "\n",
    "    Returns:\n",
    "        dict: flatten dict of the labelbox object\n",
    "        dict: flatten dict containing the positive labels for each patient\n",
    "    \"\"\"\n",
    "    dict_flatten = dict()\n",
    "    dict_labels = dict()\n",
    "    for annotation in tqdm(label_box_dict):\n",
    "\n",
    "        temp_dict_flatten = dict()\n",
    "        temp_dict_labels = dict()\n",
    "\n",
    "        temp_dict = dict()\n",
    "\n",
    "        name = annotation['data_row']['external_id'].split('.')[0]\n",
    "        for key, values in annotation['projects']['cl8vxju2k0z0q07ztfyt5dr7l']['labels'][0].items():\n",
    "            if isinstance(values, dict) and key == 'annotations':\n",
    "                print(values)\n",
    "                d_ = flatten_labels(values)\n",
    "                temp_dict_flatten.update({key:d_})\n",
    "                temp_dict_labels.update({key:d_})\n",
    "            else: \n",
    "                temp_dict_flatten.update({key:values})\n",
    "\n",
    "        dict_flatten.update({name:dict_flatten})\n",
    "        dict_labels.update({name:temp_dict_labels})\n",
    "        display(dict_labels)\n",
    "        dict_labels = get_single_value(adjust_name(dict_labels))\n",
    "        one_hot_enconded, feature_list = get_hot_labels(dict_labels)\n",
    "    return dict_flatten, dict_labels, one_hot_enconded, feature_list, pd.DataFrame.from_dict(one_hot_enconded,orient='index',columns=feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def generate_cleaned_dict_v2_no_tqdm(label_box_data):\n",
    "    \"\"\"\n",
    "    Processes Labelbox data to extract and flatten labels, and generates one-hot encoded vectors.\n",
    "    Modified to work without tqdm for progress visualization.\n",
    "\n",
    "    Parameters:\n",
    "        label_box_data (list): List of Labelbox data entries.\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing dictionaries of flattened labels, positive labels, and one-hot encoded vectors,\n",
    "        along with a list of feature names and a DataFrame.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    flattened_dict = {}\n",
    "    positive_labels_dict = {}\n",
    "\n",
    "    for annotation in label_box_data:\n",
    "        name = annotation['data_row']['external_id'].split('.')[0]\n",
    "        project_labels = annotation['projects']['cl8vxju2k0z0q07ztfyt5dr7l']['labels'][0]\n",
    "        annotations = project_labels.get('annotations', {})\n",
    "\n",
    "        if annotations:\n",
    "            flattened_labels = flatten_labels(annotations)\n",
    "            positive_labels = adjust_name(get_single_value({name: {'annotations': flattened_labels}}))\n",
    "            positive_labels_dict.update(positive_labels)\n",
    "            flattened_dict[name] = project_labels\n",
    "\n",
    "    one_hot_encoded, feature_list = get_hot_labels(positive_labels_dict)\n",
    "    df_one_hot = pd.DataFrame.from_dict(one_hot_encoded, orient='index', columns=feature_list)\n",
    "\n",
    "    return flattened_dict, positive_labels_dict, one_hot_encoded, feature_list, df_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the final one-hot-encoded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_flatten, dict_labels, one_hot_enconded, feature_list, final_one_hot = generate_cleaned_dict_v2_no_tqdm(export_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list(dict_flatten.keys())\n",
    "print(len(key_list))\n",
    "print(len(final_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(final_one_hot.head(n=5))\n",
    "#display(final_one_hot.to_csv('final_one_hot.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column 'all_values' with array of all values in each row\n",
    "final_one_hot['all_values'] = final_one_hot.apply(lambda row: row.values, axis=1)\n",
    "\n",
    "# Identifying rows where all values are 0\n",
    "# Excluding the 'all_values' and 'Unnamed: 0' columns for this check\n",
    "zero_value_rows = final_one_hot[final_one_hot.drop(columns=['all_values']).eq(0).all(axis=1)].reset_index()\n",
    "\n",
    "# Displaying the rows with all zero values\n",
    "display(zero_value_rows.describe())\n",
    "display(zero_value_rows['index'])\n",
    "## ROBERT : This should be \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find the patients equivalent to those annotated\n",
    "# annotation with exact string matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cette cell itère le dataset original i.e. df_ et \n",
    "# Ajouter le one hot si le diag dans df_ match dans dict_diag\n",
    "\n",
    "dict_out_adjusted = dict()\n",
    "list_matched = list()\n",
    "## ROBERT : Used DIAGNOSIS instead of original_diagnosis\n",
    "for patient_id, in_, diag in zip(df_['RestingECG_PatientDemographics_PatientID'].tolist(),df_.index.tolist(),df_['diagnosis'].tolist()):\n",
    "    if diag in dict_diag:\n",
    "        dict_out_adjusted.update({f'{patient_id}_{in_}':one_hot_enconded[dict_diag[diag][0]]}) #ajoute le one-hot approprié\n",
    "        list_matched.append(0) #ajoute 0 pour signifier match\n",
    "\n",
    "    else:\n",
    "        dict_out_adjusted.update({f'{patient_id}_{in_}':[0] * 78}) #ajoute empty one hot de la taille appropriée\n",
    "        list_matched.append(-1) #ajoute -1 pour signifier no match\n",
    "\n",
    "# le dict resultant est transformé en df\n",
    "temp_ = pd.DataFrame.from_dict(dict_out_adjusted, orient='index',columns=feature_list)\n",
    "temp_ = temp_.reset_index()\n",
    "temp_['annotated'] = list_matched\n",
    "temp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat avec le df original pour conserver toutes les données\n",
    "temp_out = pd.concat([df_,temp_], axis=1)\n",
    "temp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(temp_out['annotated'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do a string cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proposed string cleanup\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.split(\"*** ATTENTION! mauvaise qualité de l'ECG*** \")[-1] #retiré puisque plusieurs ECG avais que ce substring comme différence\n",
    "    s = s.lower()\n",
    "    s = s.split(' ecg anormal')[0] #retiré puisque plusieurs ECG avais que ce substring comme différence\n",
    "    #s = s.lstrip()\n",
    "    #s = s.lstrip()\n",
    "    #s = s.rstrip()\n",
    "    s = unidecode(s)\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\W+\", \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the string edditing in the diag_df and df_\n",
    "df_[\"Normalized_Diag\"] = df_[\"original_diagnosis\"].apply(normalize_string)\n",
    "dict_diag = {normalize_string(k):v for k,v in dict_diag.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun the matching function\n",
    "\n",
    "dict_out_adjusted = dict()\n",
    "list_matched = list()\n",
    "for patient_id, in_, diag in zip(df_['RestingECG_PatientDemographics_PatientID'].tolist(),df_.index.tolist(),df_['Normalized_Diag'].tolist()):\n",
    "    if diag in dict_diag:\n",
    "        dict_out_adjusted.update({f'{patient_id}_{in_}':one_hot_enconded[dict_diag[diag][0]]})\n",
    "        list_matched.append(0)\n",
    "\n",
    "    else:\n",
    "        dict_out_adjusted.update({f'{patient_id}_{in_}':[0] * 78})\n",
    "        list_matched.append(-1)\n",
    "\n",
    "temp_ = pd.DataFrame.from_dict(dict_out_adjusted, orient='index',columns=feature_list)\n",
    "temp_ = temp_.reset_index()\n",
    "temp_['annotated'] = list_matched\n",
    "\n",
    "temp_out = pd.concat([df_,temp_], axis=1)\n",
    "temp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(temp_out['annotated'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_out_bad = temp_out[temp_out['annotated'] == -1]\n",
    "temp_out_bad.iloc[400]['Normalized_Diag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_out.to_parquet('/media/data1/anolin/out_v2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add other ECGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"alexis nolin-lapalme\"\n",
    "__email__ = \"alexis.nolin-lapalme@umontreal.ca\"\n",
    "\n",
    "\n",
    "import argparse\n",
    "import base64\n",
    "import os\n",
    "import struct\n",
    "from datetime import datetime\n",
    "\n",
    "# utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xmltodict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')\n",
    "\n",
    "\n",
    "if is_interactive():\n",
    "    from tqdm.notebook import tqdm\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "\n",
    "\n",
    "class tinyxml2df:\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_path: str,\n",
    "        out_path: str = \"/media/data1/anolin/ECG\",\n",
    "        verbose: bool = True,\n",
    "        save: bool = True,\n",
    "    ):\n",
    "        self.path = in_path\n",
    "        self.out_path = out_path\n",
    "        self.verbose = verbose\n",
    "        self.save = save\n",
    "\n",
    "    def remove_a_key(self, d, remove_key):\n",
    "        if isinstance(d, dict):\n",
    "            for key in list(d.keys()):\n",
    "                if key == remove_key:\n",
    "                    del d[key]\n",
    "                else:\n",
    "                    self.remove_a_key(d[key], remove_key)\n",
    "\n",
    "    def decode_ekg_muse(self, raw_wave):\n",
    "        \"\"\"\n",
    "        Ingest the base64 encoded waveforms and transform to numeric\n",
    "        \"\"\"\n",
    "        # covert the waveform from base64 to byte array\n",
    "        arr = base64.b64decode(bytes(raw_wave, \"utf-8\"))\n",
    "\n",
    "        # unpack every 2 bytes, little endian (16 bit encoding)\n",
    "        unpack_symbols = \"\".join([char * (len(arr) // 2) for char in \"h\"])\n",
    "        byte_array = struct.unpack(unpack_symbols, arr)\n",
    "        return byte_array\n",
    "\n",
    "    def decode_ekg_muse_to_array(self, raw_wave, downsample=1):\n",
    "        \"\"\"\n",
    "        Ingest the base64 encoded waveforms and transform to numeric\n",
    "        downsample: 0.5 takes every other value in the array. Muse samples at 500/s and the sample model requires 250/s. So take every other.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            dwnsmpl = int(1 // downsample)\n",
    "        except ZeroDivisionError:\n",
    "            print(\"You must downsample by more than 0\")\n",
    "        # covert the waveform from base64 to byte array\n",
    "        arr = base64.b64decode(bytes(raw_wave, \"utf-8\"))\n",
    "\n",
    "        # unpack every 2 bytes, little endian (16 bit encoding)\n",
    "        unpack_symbols = \"\".join([char * int(len(arr) / 2) for char in \"h\"])\n",
    "        byte_array = struct.unpack(unpack_symbols, arr)\n",
    "        return np.array(byte_array)[::dwnsmpl]\n",
    "\n",
    "    def xml_to_np_array_file(self, dic, path_to_output=os.getcwd()):\n",
    "        \"\"\"\n",
    "        Upload the ECG as numpy array with shape=[2500,12,1] ([time, leads, 1]).\n",
    "        The voltage unit should be in 1 mv/unit and the sampling rate should be 250/second (total 10 second).\n",
    "        The leads should be ordered as follow I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6.\n",
    "        \"\"\"\n",
    "        # print(dic)\n",
    "        try:\n",
    "            pt_id = dic[\"RestingECG\"][\"PatientDemographics\"][\"PatientID\"]\n",
    "        except:\n",
    "            print(\"no PatientID\")\n",
    "            pt_id = \"none\"\n",
    "        try:\n",
    "            AcquisitionDateTime = (\n",
    "                dic[\"RestingECG\"][\"TestDemographics\"][\"AcquisitionDate\"]\n",
    "                + \"_\"\n",
    "                + dic[\"RestingECG\"][\"TestDemographics\"][\"AcquisitionTime\"].replace(\":\", \"-\")\n",
    "            )\n",
    "        except:\n",
    "            print(\"no AcquisitionDateTime\")\n",
    "            AcquisitionDateTime = \"none\"\n",
    "\n",
    "        # try:\n",
    "        #     requisition_number = dic['RestingECG']['Order']['RequisitionNumber']\n",
    "        # except:\n",
    "        #     print(\"no requisition_number\")\n",
    "        #     requisition_number = \"none\"\n",
    "\n",
    "        # need to instantiate leads in the proper order for the model\n",
    "        lead_order = [\n",
    "            \"I\",\n",
    "            \"II\",\n",
    "            \"III\",\n",
    "            \"aVR\",\n",
    "            \"aVL\",\n",
    "            \"aVF\",\n",
    "            \"V1\",\n",
    "            \"V2\",\n",
    "            \"V3\",\n",
    "            \"V4\",\n",
    "            \"V5\",\n",
    "            \"V6\",\n",
    "        ]\n",
    "\n",
    "        \"\"\"\n",
    "        Each EKG will have this data structure:\n",
    "        lead_data = {\n",
    "            'I': np.array\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "        lead_data = dict.fromkeys(lead_order)\n",
    "        # lead_data = {leadid: None for k in lead_order}\n",
    "\n",
    "        #     for all_lead_data in dic['RestingECG']['Waveform']:\n",
    "        #         for single_lead_data in lead['LeadData']:\n",
    "        #             leadname =  single_lead_data['LeadID']\n",
    "        #             if leadname in (lead_order):\n",
    "        try:\n",
    "            for lead in dic[\"RestingECG\"][\"Waveform\"]:\n",
    "                for leadid in range(len(lead[\"LeadData\"])):\n",
    "                    sample_length = len(\n",
    "                        self.decode_ekg_muse_to_array(lead[\"LeadData\"][leadid][\"WaveFormData\"])\n",
    "                    )\n",
    "                    # sample_length is equivalent to dic['RestingECG']['Waveform']['LeadData']['LeadSampleCountTotal']\n",
    "                    if sample_length == 5000:\n",
    "                        lead_data[\n",
    "                            lead[\"LeadData\"][leadid][\"LeadID\"]\n",
    "                        ] = self.decode_ekg_muse_to_array(\n",
    "                            lead[\"LeadData\"][leadid][\"WaveFormData\"], downsample=0.5\n",
    "                        )\n",
    "                    elif sample_length == 2500:\n",
    "                        lead_data[\n",
    "                            lead[\"LeadData\"][leadid][\"LeadID\"]\n",
    "                        ] = self.decode_ekg_muse_to_array(\n",
    "                            lead[\"LeadData\"][leadid][\"WaveFormData\"], downsample=1\n",
    "                        )\n",
    "                    else:\n",
    "                        continue\n",
    "                # ensures all leads have 2500 samples and also passes over the 3 second waveform\n",
    "\n",
    "            lead_data[\"III\"] = np.array(lead_data[\"II\"]) - np.array(lead_data[\"I\"])\n",
    "            lead_data[\"aVR\"] = -(np.array(lead_data[\"I\"]) + np.array(lead_data[\"II\"])) / 2\n",
    "            lead_data[\"aVF\"] = (np.array(lead_data[\"II\"]) + np.array(lead_data[\"III\"])) / 2\n",
    "            lead_data[\"aVL\"] = (np.array(lead_data[\"I\"]) - np.array(lead_data[\"III\"])) / 2\n",
    "\n",
    "            lead_data = {k: lead_data[k] for k in lead_order}\n",
    "            # drops V3R, V4R, and V7 if it was a 15-lead ECG\n",
    "\n",
    "            # now construct and reshape the array\n",
    "            # converting the dictionary to an np.array\n",
    "            temp = []\n",
    "            for key, value in lead_data.items():\n",
    "                temp.append(value)\n",
    "\n",
    "            # transpose to be [time, leads, ]\n",
    "            ekg_array = np.array(temp).T\n",
    "\n",
    "            # expand dims to [time, leads, 1]\n",
    "            ekg_array = np.expand_dims(ekg_array, axis=-1)\n",
    "\n",
    "            # Here is a check to make sure all the model inputs are the right shape\n",
    "            #     assert ekg_array.shape == (2500, 12, 1), \"ekg_array is shape {} not (2500, 12, 1)\".format(ekg_array.shape )\n",
    "\n",
    "            # filename = '/ekg_waveform_{}_{}.npy'.format(pt_id, requisition_number)\n",
    "            filename = f\"{pt_id}_{AcquisitionDateTime}.npy\"\n",
    "\n",
    "            path_to_output += filename\n",
    "            # print(path_to_output)\n",
    "            with open(path_to_output, \"wb\") as f:\n",
    "                np.save(f, ekg_array)\n",
    "            return path_to_output\n",
    "\n",
    "        except:\n",
    "            print(\"error\", dic)\n",
    "            return None\n",
    "\n",
    "    def flatten(self, input_node: dict, key_: str = \"\", output_dict: dict = {}):\n",
    "        self.remove_a_key(input_node, \"Waveform\")\n",
    "        self.remove_a_key(input_node, \"OriginalDiagnosis\")\n",
    "        self.remove_a_key(input_node, \"Diagnosis\")\n",
    "\n",
    "        if isinstance(input_node, dict):\n",
    "            for key, val in input_node.items():\n",
    "                new_key = f\"{key_}_{key}\" if key_ else f\"{key}\"\n",
    "                self.flatten(val, new_key, output_dict)\n",
    "        elif isinstance(input_node, list):\n",
    "            for idx, item in enumerate(input_node):\n",
    "                self.flatten(item, f\"{key_}_{idx}\", output_dict)\n",
    "        else:\n",
    "            output_dict[key_] = input_node\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def check_abnoramlity(self, data: pd.DataFrame):\n",
    "        warn = [\"Analyse impossible\", \"ECG anormal\"]\n",
    "        list_abnormality = [0] * data.shape[0]\n",
    "        for pos, entry in enumerate(data[\"original_diagnosis\"].values):\n",
    "            if any(x in entry for x in warn):\n",
    "                list_abnormality[pos] = -1\n",
    "\n",
    "        for pos, entry in enumerate(data[\"diagnosis\"].values):\n",
    "            if any(x in entry for x in warn):\n",
    "                list_abnormality[pos] = -1\n",
    "\n",
    "        data[\"warnings\"] = list_abnormality\n",
    "        return data\n",
    "\n",
    "    def read2flatten(self):\n",
    "        xml_dict_list = list()\n",
    "        path_list = list()\n",
    "        xml_list = list()\n",
    "        extracted = list()\n",
    "        npy_list = list()\n",
    "        dx_txt_list = list()\n",
    "        original_dx_txt_list = list()\n",
    "\n",
    "        # print(self.path)\n",
    "        # files_with_xml = self.path.apply(lambda path: [_ for _ in os.listdir(path) if _.endswith('.xml')]).sum()\n",
    "        ## Make directory self.out_path if it doesn't exist\n",
    "        if not os.path.exists(self.out_path):\n",
    "            os.makedirs(self.out_path)\n",
    "        if not os.path.exists(os.path.join(self.out_path, \"ecg_npy/\")):\n",
    "            os.makedirs(os.path.join(self.out_path, \"ecg_npy/\"))\n",
    "            print(\"Creating directory\")\n",
    "\n",
    "        # iterate through all the files name verbose or not\n",
    "        # print(\"{} | Currently transforming {} xml files from dir {} into dict\".format(datetime.now().strftime(\"%H:%M:%S\"),len(files_with_xml),self.path))\n",
    "        list_files = os.listdir(self.path)\n",
    "        for file_xml in tqdm(\n",
    "           list_files, total=len(list_files), desc=\"Transforming xml files into dict\"\n",
    "        ):\n",
    "            # with open(os.path.join(self.path,file_xml), 'r') as xml:\n",
    "            with open(os.path.join(self.path,file_xml)) as xml:\n",
    "                path_list.append(os.path.join(self.path,file_xml))\n",
    "                # load\n",
    "                # *|MARKER_CURSOR|*\n",
    "                ECG_data_nested = xmltodict.parse(xml.read())\n",
    "                npy_extracted = self.xml_to_np_array_file(\n",
    "                    ECG_data_nested, os.path.join(self.out_path, \"ecg_npy/\")\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    dx_txt = []\n",
    "                    for line in ECG_data_nested[\"RestingECG\"][\"Diagnosis\"][\"DiagnosisStatement\"]:\n",
    "                        dx_txt.append(line[\"StmtText\"])\n",
    "                    ## Flatten array dx_txt and add whitespace between each element\n",
    "                    dx_txt = \" \".join(dx_txt)\n",
    "                    dx_txt_list.append(dx_txt)\n",
    "                except:\n",
    "                    # print(ECG_data_nested)\n",
    "                    dx_txt_list.append(\"-1\")\n",
    "                try:\n",
    "                    original_dx_txt = []\n",
    "                    for line in ECG_data_nested[\"RestingECG\"][\"OriginalDiagnosis\"][\n",
    "                        \"DiagnosisStatement\"\n",
    "                    ]:\n",
    "                        original_dx_txt.append(line[\"StmtText\"])\n",
    "                    original_dx_txt = \" \".join(original_dx_txt)\n",
    "                    original_dx_txt_list.append(original_dx_txt)\n",
    "                except:\n",
    "                    original_dx_txt_list.append(\"-1\")\n",
    "\n",
    "                ECG_data_flatten = self.flatten(ECG_data_nested)\n",
    "\n",
    "                # append to the list\n",
    "                ECG_extracted = xml_dict_list.append(ECG_data_flatten.copy())\n",
    "                if npy_extracted == None:\n",
    "                    extracted.append(\"False\")\n",
    "                    npy_list.append(\"Error\")\n",
    "                else:\n",
    "                    extracted.append(\"True\")\n",
    "                    npy_list.append(npy_extracted)\n",
    "\n",
    "                xml_list.append(os.path.join(self.path,file_xml))\n",
    "\n",
    "        df = pd.DataFrame(xml_dict_list)\n",
    "        df[\"diagnosis\"] = dx_txt_list\n",
    "        df[\"original_diagnosis\"] = original_dx_txt_list\n",
    "        df[\"xml_path\"] = xml_list\n",
    "        df[\"npy_path\"] = npy_list\n",
    "        df[\"extracted\"] = extracted\n",
    "        df = self.check_abnoramlity(df)\n",
    "\n",
    "        if self.save == True:\n",
    "            df.to_csv(\n",
    "                os.path.join(\n",
    "                    self.out_path,\n",
    "                    \"df_xml_{}_n_{}.csv\".format(datetime.now().strftime(\"%Y_%m_%d\"), df.shape[0]),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "out_new = tinyxml2df('/media/data1/anolin/out_ECG_latest/xml/', '/media/data1/anolin/', True, True).read2flatten()\n",
    "\n",
    "out_new.to_csv(\"temp_new_xml.csv\")\n",
    "\n",
    "\n",
    "out_new = pd.read_csv(\"temp_new_xml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_new = pd.read_csv(\"temp_new_xml.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_combined = pd.concat([df_,out_new], axis=0)\n",
    "new_out_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = s.split(\"*** ATTENTION! mauvaise qualité de l'ECG*** \")[-1] #retiré puisque plusieurs ECG avais que ce substring comme différence\n",
    "    s = s.lower()\n",
    "    s = s.split(' ecg anormal')[0] #retiré puisque plusieurs ECG avais que ce substring comme différence\n",
    "    #s = s.lstrip()\n",
    "    #s = s.lstrip()\n",
    "    #s = s.rstrip()\n",
    "    s = unidecode(s)\n",
    "    s = s.strip()\n",
    "    s = re.sub(r\"\\W+\", \" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_combined[\"Normalized_Diag\"] = new_out_combined[\"original_diagnosis\"].apply(normalize_string)\n",
    "\n",
    "dict_diag = {normalize_string(k):v for k,v in dict_diag.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_combined_dict_out_adjusted = dict()\n",
    "new_out_combined_list_matched = list()\n",
    "counter = 0\n",
    "for patient_id, in_, diag in zip(new_out_combined['RestingECG_PatientDemographics_PatientID'].tolist(),new_out_combined.index.tolist(),new_out_combined['Normalized_Diag'].tolist()):\n",
    "    if diag in dict_diag:\n",
    "        new_out_combined_dict_out_adjusted.update({f'{patient_id}_{in_}_{counter}':out_dict[dict_diag[diag][0]]})\n",
    "        new_out_combined_list_matched.append(0)\n",
    "\n",
    "    else:\n",
    "        new_out_combined_dict_out_adjusted.update({f'{patient_id}_{in_}_{counter}':[0] * 78})\n",
    "        new_out_combined_list_matched.append(-1)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "temp_new_out = pd.DataFrame.from_dict(new_out_combined_dict_out_adjusted, orient='index',columns=list_features)\n",
    "temp_new_out = temp_new_out.reset_index()\n",
    "\n",
    "print(len(new_out_combined_list_matched))\n",
    "temp_new_out['annotated'] = new_out_combined_list_matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_new_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_combined = new_out_combined.reset_index(drop=True)\n",
    "temp_new_out = temp_new_out.reset_index(drop=True)\n",
    "final_all_out = pd.concat([new_out_combined,temp_new_out], axis=1)\n",
    "final_all_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counter({-1: 497339, 0: 1396046})\n",
    "\n",
    "Counter(final_all_out['annotated'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1396046/(1396046+587706)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_all_out.to_csv('/media/data1/anolin/added_label_box_2M.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_all_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
